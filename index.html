
<html>
  <head>
    <title> Enric Boix-Adsera </title>
	<style type="text/css">
    		body{
    		    /* line-height: 1.5; */
    		    padding: 4em 1em;
    		    width:1000px;
    		    margin-left:auto;
    		    margin-right:auto;
		    font-size:1.05em;
    		}
		li{
 	   	    margin: .5em 0;
		}
	</style>
  </head>
  <body>
    <img src="profile2.png" style="float:right;height:250px" hspace="20px">
    <h1> Enric Boix-Adsera </h1>
    <p>I am an assistant professor at <a href="https://statistics.wharton.upenn.edu/">Wharton Statistics and Data Science</a> in the University of Pennsylvania.

    <p>

My research focuses on building a mathematical science of deep learning. My goal is to understand the fundamental mechanisms driving how neural networks learn, in order to enable more efficient and more trustworthy AI systems.


      <!-- My interests include deep learning, high-dimensional statistics, optimal transport, and average-case complexity. -->
    </p>

    <p>
      <b>Teaching:</b> In Spring 2026, I am teaching a newly-designed course on Foundations of AI. Please see the syllabus  <a href="https://eboix.github.io/Foundations_of_AI.pdf">here</a>.
    </p>

    <p>
      <b>PhD Research opportunities:</b> Please <a href="https://www.applyweb.com/upennw/index.ftl">apply to the Wharton Statistics and Data Science PhD program</a> and mention me on your application if you are interested in working with me. This will ensure that I can review your application.
    </p>
    
  <p>
      <b>Email:</b> <a href="mailto:eboix@wharton.upenn.edu">eboix@wharton.upenn.edu</a>
  </p>

    </p>
      <b>Bio:</b> Previously (in reverse-chronological order) I completed a postdoc at MIT Math and Harvard CMSA, was a Simons Fellow in the Large Language Models program at UC Berkeley, and obtained a PhD in EECS at MIT advised by Guy Bresler and Philippe Rigollet. During my PhD I was generously supported by an NSF Graduate Research Fellowship, a Siebel Fellowship, and an Apple AI/ML fellowship.  I received my undergraduate degree in mathematics from Princeton University, where I was advised by Emmanuel Abbe.
    </p>

    <p>
      <b>People:</b> I am fortunate to work with a number of amazing students and collaborators. At UPenn, I am currently co-advising Honam Wong with Surbhi Goel.
    </p>
    
	<h4 class="by-year-view">Publications [sorted by year | <a href="#" class="by-topic-btn">sorted by topic</a>]
</h4>
<h4 class="by-topic-view" hidden>Publications [<a href="#" class="by-year-btn">sorted by year</a> | sorted by topic]
</h4>

  <div class = "by-year-view">

    <p>* denotes equally-contributing first authors and (&alpha;&beta;) denotes alphabetical order </p>

    <div class = "year">2025</div>
    <ul>

      <li>
        <a href="https://arxiv.org/abs/2507.05644" class="publications">The Features at Convergence Theorem: a first-principles alternative to the Neural Feature Ansatz for how networks learn representations</a>
         <br>
         <b>EB</b>*, Neil Mallinar*, James B. Simon, Mikhail Belkin.
         <br>
         <em>Preprint.</em>
      </li>

      <li>
        <a href="https://arxiv.org/abs/2505.06839" class="publications">The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts</a>
         <br>
         (&alpha;&beta;) <b>EB</b>, Philippe Rigollet.
         <br>
         <em>Preprint.</em>
      </li>
      
      <li>
        <a href="https://arxiv.org/abs/2505.21825" class="publications"> Let Me Think! A long chain of thought can be worth exponentially many short ones</a>
         <br>
          Parsa Mirtaheri*, Ezra Edelman*, Samy Jelassi, Eran Malach, <b>EB</b>.
         <br>
         <em>Conference on Neural Information Processing Systems (NeurIPS'25).</em>
      </li>

      <li>
        <a href="https://arxiv.org/abs/2502.03708" class="publications"> Toward universal steering and monitoring of AI models</a>
         <br>
          Daniel Beaglehole, Adityanarayanan Radhakrishnan, <b>EB</b>, Mikhail Belkin.
         <br>
         <em>Preprint.</em>
      </li>
      <li>
        <a href="https://arxiv.org/abs/2501.19149" class="publications"> On the inductive bias of infinite-depth ResNets and the bottleneck rank</a>
         <br>
          <b>EB</b>.
         <br>
         <em>Preprint.</em>
      </li>
    </ul>

    <div class = "year">2024</div>
    <ul>
      <li>
        <a href="https://arxiv.org/abs/2403.09053" class="publications"> Towards a theory of model distillation</a>
         <br>
          <b>EB</b>.
         <br>
         <em>Preprint.</em>
      </li>
    </ul>

	<div class = "year">2023</div>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/2310.09753" class="publications"> When can transformers reason with abstract symbols?</a>
       <br>
        <b>EB</b>*, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, Joshua Susskind.
       <br>
       <em>International Conference on Learning Representations (ICLR'24).</em>
    </li>

    <li>
      <a href="https://arxiv.org/abs/2311.07064" class="publications">Prompts have evil twins</a>
       <br>
        Rimon Melamed, Lucas H. McCabe, Tanay Wakhare, Yejin Kim, H. Howie Huang, <b>EB</b>.
       <br>
       <em>Conference on Empirical Methods in Natural Language Processing (EMNLP'24).</em>
    </li>

    <li>
      <a href="https://arxiv.org/abs/2306.07042" class="publications"> Transformers learn through gradual rank increase</a>
       <br>
        <b>EB</b>*, Etai Littwin*, Emmanuel Abbe, Samy Bengio, Joshua Susskind.
       <br>
       <em>Conference on Neural Information Processing Systems (NeurIPS'23).</em>
    </li>

    <li>
      <a href="https://arxiv.org/abs/2305.13141" class="publications"> Tight conditions for when the NTK approximation is valid</a>
       <br>
        (&alpha;&beta;) <b>EB</b>, Etai Littwin.
       <br>
       <em>Transactions on Machine Learning Research (TMLR).</em>
    </li>

    <li>
      <a href="https://arxiv.org/abs/2302.11055" class="publications"> SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics</a>
       <br>
        (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>, Theodor Misiakiewicz.
       <br>
       <em>Conference on Learning Theory (COLT'23).</em>
    </li>
  </ul>

	<div class = "year">2022</div>

	<ul>
		<li>
      <a href="https://arxiv.org/abs/2210.06545" class="publications"> GULP: a prediction-based metric between representations</a>
       <br>
        <b>EB</b>*, Hannah Lawrence*, George Stepaniants*, Philippe Rigollet.
       <br>
       <em>Conference on Neural Information Processing Systems (NeurIPS'22).</em>
       <br>
       <b>Selected as oral</b> (top 8% accepted papers)
    </li>

    <li>
      <a href="https://arxiv.org/abs/2208.03113" class="publications"> On the non-universality of deep learning: quantifying the cost of symmetry</a>
       <br>
        (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>.
       <br>
       <em>Conference on Neural Information Processing Systems (NeurIPS'22).</em>
    </li>

    <li>
 <a href="https://arxiv.org/abs/2202.08658" class="publications"> The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks</a>
      <br>
       (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>, Theodor Misiakiewicz.
      <br>
      <em>Conference on Learning Theory (COLT'22).</em>
		</li>
	</ul>

<div class="year">2021</div>
	<ul>
    <li>
      <a href="https://arxiv.org/abs/2108.10573" class="publications"> The staircase property: How hierarchical structure can guide deep learning</a>
      <br>
       (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>, Matthew Brennan, Guy Bresler, Dheeraj Nagaraj.
      <br>
      <em>Conference on Neural Information Processing Systems (NeurIPS'21).</em>
    </li>
    <li>
      <a href="https://arxiv.org/abs/2106.03969" class="publications"> Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models</a>
      <br>
       (&alpha;&beta;) <b>EB</b>, Guy Bresler, Frederic Koehler.
      <br>
      <em>Foundations of Computer Science (FOCS'21).</em>
    </li>
		<li>
			<a href="https://arxiv.org/abs/2101.01100" class="publications"> Wasserstein barycenters are NP-hard to compute</a>
      <br>
       (&alpha;&beta;) Jason Altschuler, <b>EB</b>.
      <br>
      <em>SIAM Journal on Mathematics of Data Science (SIMODS)</em>.
		</li>

	</ul><div class="year">2020</div>
	<ul>
		<li>
			<a href="https://arxiv.org/abs/2012.05398" class="publications"> Hardness results for Multimarginal Optimal Transport problems</a>
      <br>
      (&alpha;&beta;) Jason Altschuler, <b>EB</b>.
      <br>
      <em>Discrete Optimization (DISOPT).</em>
		</li>
		<li>
			<a href="https://arxiv.org/abs/2008.03006" class="publications"> Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure</a>
      <br>
      (&alpha;&beta;) Jason Altschuler, <b>EB</b>.
      <br>
      <em>Mathematical Programming.</em>
		</li>
		<li>
      <a href="https://arxiv.org/abs/2006.08012" class="publications"> Wasserstein barycenters can be computed in polynomial time in fixed dimension</a>
      <br>
      (&alpha;&beta;) Jason Altschuler, <b>EB</b>.
      <br>
      <em>Journal of Machine Learning Research (JMLR).</em>
		</li>
		<li>
      <a href="https://arxiv.org/abs/2002.05240" class="publications"> The Multiplayer Colonel Blotto Game</a>
      <br>
      (&alpha;&beta;) <b>EB</b>, Ben Edelman, Siddhartha Jayanti.
      <br>
      <em>Conference version: Economics and Computation (EC'20).</em>
      <br>
      <em>Journal version: Games and Economic Behavior.</em>
		</li>

	</ul><div class="year">2019</div>
	<ul>
		<li>
      <a href="https://arxiv.org/abs/1903.08247" class="publications"> The Average-Case Complexity of Counting Cliques in Erdos-Renyi Hypergraphs</a>
      <br>
      (&alpha;&beta;) <b>EB</b>, Matthew Brennan, Guy Bresler.
      <br>
      <em>Foundations of Computer Science (FOCS'19).</em>
      <br>
      <b>Invited to the SIAM Journal on Computing Special Issue for FOCS 2019</b>
		</li>
		<li>
      <a href="http://papers.nips.cc/paper/9575-sample-efficient-active-learning-of-causal-trees" class="publications"> Sample-Efficient Active Learning of Causal Trees</a>
      <br>
      Kristjan Greenewald*, Dmitriy Katz-Rogozhnikov*, Karthikeyan Shanmugam*, Sara Magliacane, Murat Kocaoglu, <b>EB</b>, Guy Bresler.
      <br>
      <em>Conference on Neural Information Processing Systems (NeurIPS'19).</em>
		</li>
		<li>
      <a href="https://ieeexplore.ieee.org/abstract/document/8849658" class="publications"> Subadditivity Beyond Trees and the Chi-Squared Mutual Information</a>
      <br>
      (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>.
      <br>
      <em>IEEE International Symposium on Information Theory (ISIT'19).</em>
		</li>
    <li>
      <a href="https://dl.acm.org/doi/10.1145/3293611.3331593" class="publications"> Randomized Concurrent Set Union and Generalized Wake-Up</a>
      <br>
      Siddhartha Jayanti*, Robert E. Tarjan*, <b>EB</b>.
      <br>
      <em>Symposium on Principles of Distributed Computing (PODC'19).</em>
    </li>
	</ul>

  <div class="year">2018</div>
	<ul>
		<li>
			<a href="https://projecteuclid.org/euclid.aoap/1596009617" class="publications"> An Information-Percolation Bound for Spin Synchronization on General Graphs</a>
      <br>
      (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>.
      <br><em>Annals of Applied Probability (AAP).</em>
		</li>
		<li>
			<a href="https://epubs.siam.org/doi/abs/10.1137/19M1257135" class="publications"> Graph powering and spectral robustness</a>
      <br>
      (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>, Peter Ralli, Colin Sandon.
      <br>
			<em>SIAM Journal on Mathematics of Data Science (SIMODS).</em>
		</li>
	</ul>
  </div>
</div>

<div class = "by-topic-view" hidden>

      <p>* denotes equally-contributing first authors and (&alpha;&beta;) denotes alphabetical order </p>

  <div class="topic">Learning</div>
    <ul>

      <li>
        <a href="https://arxiv.org/abs/2507.05644" class="publications">The Features at Convergence Theorem: a first-principles alternative to the Neural Feature Ansatz for how networks learn representations</a>
         <br>
         <b>EB</b>*, Neil Mallinar*, James B. Simon, Mikhail Belkin.
         <br>
         <em>Preprint.</em>
      </li>

      <li>
        <a href="https://arxiv.org/abs/2505.06839" class="publications">The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts</a>
         <br>
         (&alpha;&beta;) <b>EB</b>, Philippe Rigollet.
         <br>
         <em>Preprint.</em>
      </li>
      
      <li>
        <a href="https://arxiv.org/abs/2505.21825" class="publications"> Let Me Think! A long chain of thought can be worth exponentially many short ones</a>
         <br>
          Parsa Mirtaheri*, Ezra Edelman*, Samy Jelassi, Eran Malach, <b>EB</b>.
         <br>
         <em>Conference on Neural Information Processing Systems (NeurIPS'25).</em>
      </li>

      <li>
        <a href="https://arxiv.org/abs/2502.03708" class="publications"> Toward universal steering and monitoring of AI models</a>
         <br>
          Daniel Beaglehole, Adityanarayanan Radhakrishnan, <b>EB</b>, Mikhail Belkin.
         <br>
         <em>Preprint.</em>
      </li>
      <li>
        <a href="https://arxiv.org/abs/2501.19149" class="publications"> On the inductive bias of infinite-depth ResNets and the bottleneck rank</a>
         <br>
          <b>EB</b>.
         <br>
         <em>Preprint.</em>
      </li>

        <li>
          <a href="https://arxiv.org/abs/2403.09053" class="publications"> Towards a theory of model distillation</a>
           <br>
            <b>EB</b>.
           <br>
           <em>Preprint.</em>
        </li>

      <li>
        <a href="https://arxiv.org/abs/2310.09753" class="publications"> When can transformers reason with abstract symbols?</a>
         <br>
          <b>EB</b>*, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, Joshua Susskind.
         <br>
         <em>International Conference on Learning Representations (ICLR'24).</em>
      </li>

      <li>
        <a href="https://arxiv.org/abs/2311.07064" class="publications">Prompts have evil twins</a>
         <br>
          Rimon Melamed, Lucas H. McCabe, Tanay Wakhare, Yejin Kim, H. Howie Huang, <b>EB</b>.
         <br>
         <em>Conference in Empirical Methods in Natural Language Processing (EMNLP'24).</em>
      </li>

      <li>
        <a href="https://arxiv.org/abs/2306.07042" class="publications"> Transformers learn through gradual rank increase</a>
         <br>
          <b>EB</b>*, Etai Littwin*, Emmanuel Abbe, Samy Bengio, Joshua Susskind.
         <br>
         <em>Conference on Neural Information Processing Systems (NeurIPS'23).</em>
      </li>

      <li>
        <a href="https://arxiv.org/abs/2305.13141" class="publications"> Tight conditions for when the NTK approximation is valid</a>
         <br>
          (&alpha;&beta;) <b>EB</b>, Etai Littwin.
         <br>
         <em>Transactions on Machine Learning Research (TMLR).</em>
      </li>

        <li>
          <a href="https://arxiv.org/abs/2302.11055" class="publications"> SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics</a>
           <br>
           (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>, Theodor Misiakiewicz.
           <br>
           <em>Conference on Learning Theory (COLT'23).</em>
        </li>

        <li>
          <a href="https://arxiv.org/abs/2210.06545" class="publications"> GULP: a prediction-based metric between representations</a>
           <br>
           <b>EB</b>*, Hannah Lawrence*, George Stepaniants*, Philippe Rigollet.
           <br>
           <em>Conference on Neural Information Processing Systems (NeurIPS'22).</em>
           <br>
           <b>Selected as oral</b> (top 8% accepted papers)
        </li>

        <li>
          <a href="https://arxiv.org/abs/2208.03113" class="publications"> On the non-universality of deep learning: quantifying the cost of symmetry</a>
           <br>
            (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>.
           <br>
           <em>Conference on Neural Information Processing Systems (NeurIPS'22).</em>
        </li>

      <li>
      <a href="https://arxiv.org/abs/2202.08658" class="publications"> The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks</a>
          <br>
           (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>, Theodor Misiakiewicz.
          <br>
          <em>Conference on Learning Theory (COLT'22).</em>
      </li>


      <li>
        <a href="https://arxiv.org/abs/2108.10573" class="publications"> The staircase property: How hierarchical structure can guide deep learning</a>
        <br>
         (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>, Matthew Brennan, Guy Bresler, Dheeraj Nagaraj.
        <br>
        <em>Conference on Neural Information Processing Systems (NeurIPS'21).</em>
      </li>

        <li>
          <a href="https://arxiv.org/abs/2106.03969" class="publications"> Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models</a>
          <br>
           (&alpha;&beta;) <b>EB</b>, Guy Bresler, Frederic Koehler.
          <br>
          <em>Foundations of Computer Science (FOCS'21).</em>
        </li>

      <li>
        <a href="http://papers.nips.cc/paper/9575-sample-efficient-active-learning-of-causal-trees" class="publications"> Sample-Efficient Active Learning of Causal Trees</a>
        <br>
        Kristjan Greenewald*, Dmitriy Katz-Rogozhnikov*, Karthikeyan Shanmugam*, Sara Magliacane, Murat Kocaoglu, <b>EB</b>, Guy Bresler.
        <br>
        <em>Conference on Neural Information Processing Systems (NeurIPS'19).</em>
      </li>

      <li>
        <a href="https://ieeexplore.ieee.org/abstract/document/8849658" class="publications"> Subadditivity Beyond Trees and the Chi-Squared Mutual Information</a>
        <br>
        (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>.
        <br>
        <em>IEEE International Symposium on Information Theory (ISIT'19).</em>
      </li>
      <li>
        <a href="https://projecteuclid.org/euclid.aoap/1596009617" class="publications"> An Information-Percolation Bound for Spin Synchronization on General Graphs</a>
        <br>
        (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>.
        <br><em>Annals of Applied Probability (AAP).</em>
      </li>
      <li>
        <a href="https://epubs.siam.org/doi/abs/10.1137/19M1257135" class="publications"> Graph powering and spectral robustness</a>
        <br>
        (&alpha;&beta;) Emmanuel Abbe, <b>EB</b>, Peter Ralli, Colin Sandon.
        <br>
        <em>SIAM Journal on Mathematics of Data Science (SIMODS).</em>
      </li>
    </ul>

<div class="topic">Optimal Transport</div>
<ul>
  <li>
    <a href="https://arxiv.org/abs/2101.01100" class="publications"> Wasserstein barycenters are NP-hard to compute</a>
    <br>
     (&alpha;&beta;) Jason Altschuler, <b>EB</b>.
    <br>
    <em>SIAM Journal on Mathematics of Data Science (SIMODS).</em>
  </li>
  <li>
    <a href="https://arxiv.org/abs/2012.05398" class="publications"> Hardness results for Multimarginal Optimal Transport problems</a>
    <br>
    (&alpha;&beta;) Jason Altschuler, <b>EB</b>.
    <br>
    <em>Discrete Optimization (DISOPT).</em>
  </li>
  <li>
    <a href="https://arxiv.org/abs/2008.03006" class="publications"> Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure</a>
    <br>
    (&alpha;&beta;) Jason Altschuler, <b>EB</b>.
    <br>
    <em>Mathematical Programming.</em>
  </li>
  <li>
    <a href="https://arxiv.org/abs/2006.08012" class="publications"> Wasserstein barycenters can be computed in polynomial time in fixed dimension</a>
    <br>
    (&alpha;&beta;) Jason Altschuler, <b>EB</b>.
    <br>
    <em>Journal of Machine Learning Research (JMLR).</em>
  </li>
  <li>
    <a href="https://arxiv.org/abs/2002.05240" class="publications"> The Multiplayer Colonel Blotto Game</a>
    <br>
    (&alpha;&beta;) <b>EB</b>, Ben Edelman, Siddhartha Jayanti.
    <br>
    <em>Conference version: Economics and Computation (EC'20).</em>
    <br>
    <em>Journal version: Games and Economic Behavior.</em>
  </li>
</ul>

<div class="topic">Miscellaneous</div>
<ul>
<li>
  <a href="https://dl.acm.org/doi/10.1145/3293611.3331593" class="publications"> Randomized Concurrent Set Union and Generalized Wake-Up</a>
  <br>
  Siddhartha Jayanti*, Robert E. Tarjan*, <b>EB</b>.
  <br>
  <em>Symposium on Principles of Distributed Computing (PODC'19).</em>
</li>

<li>
<a href="https://arxiv.org/abs/1903.08247" class="publications"> The Average-Case Complexity of Counting Cliques in Erdos-Renyi Hypergraphs</a>
<br>
(&alpha;&beta;) <b>EB</b>, Matthew Brennan, Guy Bresler.
<br>
<em>Foundations of Computer Science (FOCS'19).</em>
<br>
<b>Invited to the SIAM Journal on Computing Special Issue for FOCS 2019</b>
</li>
</ul>
</div>
</div>

	</body>
</html>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script>
$(document).ready(function(){
    $(".by-topic-btn").click(function(){
        console.log("Hi!")
        $(".by-year-view").hide()
        $(".by-topic-view").show()
    });
    $(".by-year-btn").click(function(){
        console.log("Hi year!")
        $(".by-topic-view").hide()
        $(".by-year-view").show()
    });
});
</script>
