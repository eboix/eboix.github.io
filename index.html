<html>

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title> Enric Boix-Adsera </title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Open+Sans:wght@400;700&family=Roboto:wght@400;700&display=swap"
    rel="stylesheet">
  <style type="text/css">
    body {
      /* line-height: 1.5; */
      padding: 4em 1em;
      max-width: 750px;
      margin-left: auto;
      margin-right: auto;
      font-size: 0.9em;
      font-family: 'Open Sans', sans-serif;
    }

    li {
      margin: .5em 0;
    }

    a {
      text-decoration: none;
      color: #337ab7;
    }

    a:hover {
      text-decoration: underline;
    }

    hr {
      border: 0;
      height: 1px;
      background-color: #ccc;
    }

    code {
      font-size: 1.25em;
    }

    .profile-container {
      display: flex;
      align-items: center;
      margin-bottom: 2em;
      flex-wrap: wrap;
      justify-content: center;
    }

    .profile-image {
      height: 250px;
      margin-right: 20px;
    }

    @media (max-width: 550px) {
      .profile-container {
        flex-direction: column;
      }

      .profile-image {
        margin-right: 0;
        margin-bottom: 1em;
      }

      .profile-text {
        text-align: center;
      }
    }

    .nav-links {
      font-size: 1.2em;
      margin: 1em 0;
    }

    .contact-info {
      margin-top: 0.5em;
    }

    .expandable-box {
      border: 1px solid #ccc;
      margin-bottom: 1em;
    }

    .expandable-header {
      background-color: #f1f1f1;
      padding: 10px 15px;
      cursor: pointer;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .expandable-header:hover {
      background-color: #ddd;
    }

    .expandable-content {
      padding: 0 15px;
      max-height: 0;
      overflow: hidden;
      transition: max-height 0.2s ease-out;
    }

    .arrow {
      border: solid black;
      border-width: 0 2px 2px 0;
      display: inline-block;
      padding: 3px;
      transform: rotate(45deg);
      -webkit-transform: rotate(45deg);
      transition: transform 0.2s;
    }

    .expanded .arrow {
      transform: rotate(-135deg);
      -webkit-transform: rotate(-135deg);
    }
  </style>
</head>

<body>
  <div class="profile-container">
    <img src="profile2.png" class="profile-image">
    <div class="profile-text">
      <h1> Enric Boix-Adsera </h1>
      <p>Assistant Professor of Statistics and Data Science
        <br>
        Wharton School at University of Pennsylvania
      </p>
      <p class="contact-info"><code>eboix@wharton.upenn.edu</code>
        <br>
        <a href="https://eboix.github.io/Enric_CV.pdf">CV</a>, <a
          href="https://scholar.google.com/citations?user=KOZSrE8AAAAJ&hl=en&oi=ao">Google Scholar</a>
      </p>
    </div>
  </div>
  <hr>
  <div class="nav-links">
    <b><a href="#group">People</a> / <a href="#teaching">Teaching</a> / <a href="#papers">Papers (by Topic)</a></b>
  </div>
  <hr>

  <p>

    My research focuses on building a mathematical science of AI. My goal is to understand the fundamental mechanisms driving how neural networks learn, in order to enable more efficient and trustworthy AI systems. Recently, some topics that I have been thinking about are:
    <ol>
    <li>
      <b>AI safety</b>: How can we ensure AI systems are trustworthy as their capabilities improve? How can we monitor and control their behavior?
    </li>

    <li>
      <b>AI reasoning</b>: How can we make networks that generalize well to outputs outside of their training distribution? Why should we expect this to be possible at all?
    </li>

    <li>
      <b>AI representations</b>: What's going on inside a neural network? How does it learn internal representations that help it generalize? What makes an internal representation "useful"?
    </li>
  </ol>
  These questions are richly interconnected, and progress on one can inform the others. Beyond these themes, I am always happy to chat about new ideas and learn about new paradigms!
  </p>

  <!-- My interests include deep learning, high-dimensional statistics, optimal transport, and average-case complexity. -->

  <div id="group">
    <h2>People</h2>
    <p>
      I am very fortunate to work with many amazing people, including the students in my group:
    <ul>
      <li><a href="https://matheart.github.io/">Honam Wong</a> (UPenn PhD student coadvised with Surbhi Goel)</li>
      <li>Timothy Kong (UPenn undergraduate)</li>
    </ul>
    The group is growing! I am looking to hire creative students who have strong mathematical and empirical AI research skills. If you are interested in working with us, please see this <a
      href="https://docs.google.com/document/d/1rGgwq2U5LXxYosYGhvQ3nTKxB2f4-q4jxfDJ8UtZQsY/edit?usp=sharing" target="_blank" rel="noopener noreferrer">FAQ</a>!
    <p>
    </p>
  </div>



  <div id="teaching">
    <h2>Teaching</h2>
    <p>I designed a new course, which I am teaching for the first time in Spring 2026:
      <br> <a
        href="https://eboix.github.io/Foundations_of_AI.pdf">STAT 4850/5850 Foundations of AI: Deep Learning with
        Applications</a>.
    </p>
  </div>


  <div id="papers">
    <h2>Papers</h2>


    For papers ordered by year, please see <a
      href="https://scholar.google.com/citations?user=KOZSrE8AAAAJ&hl=en&oi=ao">Google Scholar</a>.


    <p><b>* denotes equally-contributing first authors and (&alpha;&beta;) denotes alphabetical order</b> </p>



    <div class="expandable-box">
      <div class="expandable-header">
        <span><b>AI Monitoring and Interpretability</b></span>
        <i class="arrow"></i>
      </div>
      <div class="expandable-content">
        <ul>


          <li>
            <a href="https://arxiv.org/abs/2502.03708" class="publications"> Toward universal steering and monitoring of
              AI models</a>
            <br>
            Daniel Beaglehole*, Adityanarayanan Radhakrishnan*, <b>Enric Boix-Adsera</b>, Mikhail Belkin.
            <br>
            <em>Preprint</em>, 2025.
          </li>


          <li>
            <a href="https://arxiv.org/abs/2403.09053" class="publications"> Towards a theory of model distillation</a>
            <br>
            <b>Enric Boix-Adsera</b>.
            <br>
            <em>Preprint</em>, 2024.
          </li>




          <li>
            <a href="https://arxiv.org/abs/2311.07064" class="publications">Prompts have evil twins</a>
            <br>
            Rimon Melamed, Lucas H. McCabe, Tanay Wakhare, Yejin Kim, H. Howie Huang, <b>Enric Boix-Adsera</b>.
            <br>
            <em>Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2024.
          </li>


          <li>
            <a href="https://arxiv.org/abs/2210.06545" class="publications"> GULP: a prediction-based metric between
              representations</a>
            <br>
            <b>Enric Boix-Adsera</b>*, Hannah Lawrence*, George Stepaniants*, Philippe Rigollet.
            <br>
            <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2022.
            <br>
            <b>Selected as oral</b> (top 8% accepted papers)
          </li>

        </ul>
      </div>
    </div>


    <div class="expandable-box">
      <div class="expandable-header">
        <span><b>AI Reasoning</b></span>
        <i class="arrow"></i>
      </div>
      <div class="expandable-content">
        <ul>

          <li>
            <a href="https://arxiv.org/abs/2505.21825" class="publications"> Let Me Think! A long chain of thought can
              be worth exponentially many short ones</a>
            <br>
            Parsa Mirtaheri*, Ezra Edelman*, Samy Jelassi, Eran Malach, <b>Enric Boix-Adsera</b>.
            <br>
            <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2025.
          </li>

          <li>
            <a href="https://arxiv.org/abs/2310.09753" class="publications"> When can transformers reason with abstract
              symbols?</a>
            <br>
            <b>Enric Boix-Adsera</b>*, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, Joshua Susskind.
            <br>
            <em>International Conference on Learning Representations (ICLR)</em>, 2024.
          </li>
        </ul>
      </div>
    </div>


    <div class="expandable-box">
      <div class="expandable-header">
        <span><b>How does AI learn internal representations? (Deep Learning Theory)</b></span>
        <i class="arrow"></i>
      </div>
      <div class="expandable-content">
        <ul>


          <li>
            <a href="https://arxiv.org/abs/2507.05644" class="publications">The Features at Convergence Theorem: a
              first-principles alternative to the Neural Feature Ansatz for how networks learn representations</a>
            <br>
            <b>Enric Boix-Adsera</b>*, Neil Mallinar*, James B. Simon, Mikhail Belkin.
            <br>
            <em>Preprint</em>, 2025.
          </li>

          <li>
            <a href="https://arxiv.org/abs/2505.06839" class="publications">The power of fine-grained experts:
              Granularity boosts expressivity in Mixture of Experts</a>
            <br>
            (&alpha;&beta;) <b>Enric Boix-Adsera</b>, Philippe Rigollet.
            <br>
            <em>Preprint</em>, 2025.
          </li>

          <li>
            <a href="https://arxiv.org/abs/2501.19149" class="publications"> On the inductive bias of infinite-depth
              ResNets and the bottleneck rank</a>
            <br>
            <b>Enric Boix-Adsera</b>.
            <br>
            <em>Preprint</em>, 2025.
          </li>


          <li>
            <a href="https://arxiv.org/abs/2302.11055" class="publications"> SGD learning on neural networks: leap
              complexity and saddle-to-saddle dynamics</a>
            <br>
            (&alpha;&beta;) Emmanuel Abbe, <b>Enric Boix-Adsera</b>, Theodor Misiakiewicz.
            <br>
            <em>Conference on Learning Theory (COLT)</em>, 2023.
          </li>


          <li>
            <a href="https://arxiv.org/abs/2306.07042" class="publications"> Transformers learn through gradual rank
              increase</a>
            <br>
            <b>Enric Boix-Adsera</b>*, Etai Littwin*, Emmanuel Abbe, Samy Bengio, Joshua Susskind.
            <br>
            <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2023.
          </li>

          <li>
            <a href="https://arxiv.org/abs/2305.13141" class="publications"> Tight conditions for when the NTK
              approximation is valid</a>
            <br>
            (&alpha;&beta;) <b>Enric Boix-Adsera</b>, Etai Littwin.
            <br>
            <em>Transactions on Machine Learning Research (TMLR)</em>, 2023.
          </li>

          <li>
            <a href="https://arxiv.org/abs/2208.03113" class="publications"> On the non-universality of deep learning:
              quantifying the cost of symmetry</a>
            <br>
            (&alpha;&beta;) Emmanuel Abbe, <b>Enric Boix-Adsera</b>.
            <br>
            <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2022.
          </li>


          <li>
            <a href="https://arxiv.org/abs/2202.08658" class="publications"> The merged-staircase property: a necessary
              and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks</a>
            <br>
            (&alpha;&beta;) Emmanuel Abbe, <b>Enric Boix-Adsera</b>, Theodor Misiakiewicz.
            <br>
            <em>Conference on Learning Theory (COLT)</em>, 2022.
          </li>
          <li>
            <a href="https://arxiv.org/abs/2108.10573" class="publications"> The staircase property: How hierarchical
              structure can guide deep learning</a>
            <br>
            (&alpha;&beta;) Emmanuel Abbe, <b>Enric Boix-Adsera</b>, Matthew Brennan, Guy Bresler, Dheeraj Nagaraj.
            <br>
            <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2021.
          </li>
        </ul>
      </div>
    </div>


    <div class="expandable-box">
      <div class="expandable-header">
        <span><b>Optimal transport</b></span>
        <i class="arrow"></i>
      </div>
      <div class="expandable-content">
        <ul>
          <li>
            <a href="https://arxiv.org/abs/2101.01100" class="publications"> Wasserstein barycenters are NP-hard to
              compute</a>
            <br>
            (&alpha;&beta;) Jason Altschuler, <b>Enric Boix-Adsera</b>.
            <br>
            <em>SIAM Journal on Mathematics of Data Science (SIMODS)</em>, 2022.
          </li>
          <li>
            <a href="https://arxiv.org/abs/2012.05398" class="publications"> Hardness results for Multimarginal Optimal
              Transport problems</a>
            <br>
            (&alpha;&beta;) Jason Altschuler, <b>Enric Boix-Adsera</b>.
            <br>
            <em>Discrete Optimization (DISOPT)</em>, 2021.
          </li>
          <li>
            <a href="https://arxiv.org/abs/2008.03006" class="publications"> Polynomial-time algorithms for
              Multimarginal Optimal Transport problems with structure</a>
            <br>
            (&alpha;&beta;) Jason Altschuler, <b>Enric Boix-Adsera</b>.
            <br>
            <em>Mathematical Programming</em>, 2022.
          </li>
          <li>
            <a href="https://arxiv.org/abs/2006.08012" class="publications"> Wasserstein barycenters can be computed in
              polynomial time in fixed dimension</a>
            <br>
            (&alpha;&beta;) Jason Altschuler, <b>Enric Boix-Adsera</b>.
            <br>
            <em>Journal of Machine Learning Research (JMLR)</em>, 2021.
          </li>
          <li>
            <a href="https://arxiv.org/abs/2002.05240" class="publications"> The Multiplayer Colonel Blotto Game</a>
            <br>
            (&alpha;&beta;) <b>Enric Boix-Adsera</b>, Ben Edelman, Siddhartha Jayanti.
            <br>
            <em>Conference version: Economics and Computation (EC)</em>, 2020.
            <br>
            <em>Journal version: Games and Economic Behavior</em>, 2021.
          </li>
        </ul>
      </div>
    </div>

    <div class="expandable-box">
      <div class="expandable-header">
        <span><b>Provable machine learning algorithms</b></span>
        <i class="arrow"></i>
      </div>
      <div class="expandable-content">
        <ul>
          <li>
            <a href="https://arxiv.org/abs/2106.03969" class="publications"> Chow-Liu++: Optimal Prediction-Centric
              Learning of Tree Ising Models</a>
            <br>
            (&alpha;&beta;) <b>Enric Boix-Adsera</b>, Guy Bresler, Frederic Koehler.
            <br>
            <em>Foundations of Computer Science (FOCS)</em>, 2021.
          </li>
          <li>
            <a href="http://papers.nips.cc/paper/9575-sample-efficient-active-learning-of-causal-trees"
              class="publications"> Sample-Efficient Active Learning of Causal Trees</a>
            <br>
            Kristjan Greenewald*, Dmitriy Katz-Rogozhnikov*, Karthikeyan Shanmugam*, Sara Magliacane, Murat Kocaoglu,
            <b>Enric Boix-Adsera</b>, Guy Bresler.
            <br>
            <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2019.
          </li>
          <li>
            <a href="https://projecteuclid.org/euclid.aoap/1596009617" class="publications"> An Information-Percolation
              Bound for Spin Synchronization on General Graphs</a>
            <br>
            (&alpha;&beta;) Emmanuel Abbe, <b>Enric Boix-Adsera</b>.
            <br><em>Annals of Applied Probability (AAP)</em>, 2020.
          </li>
          <li>
            <a href="https://epubs.siam.org/doi/abs/10.1137/19M1257135" class="publications"> Graph powering and
              spectral robustness</a>
            <br>
            (&alpha;&beta;) Emmanuel Abbe, <b>Enric Boix-Adsera</b>, Peter Ralli, Colin Sandon.
            <br>
            <em>SIAM Journal on Mathematics of Data Science (SIMODS)</em>, 2020.
          </li>
        </ul>
      </div>
    </div>

    <div class="expandable-box">
      <div class="expandable-header">
        <span><b>Other</b></span>
        <i class="arrow"></i>
      </div>
      <div class="expandable-content">
        <ul>
          <li>
            <a href="https://arxiv.org/abs/1903.08247" class="publications"> The Average-Case Complexity of Counting
              Cliques in Erdos-Renyi Hypergraphs</a>
            <br>
            (&alpha;&beta;) <b>Enric Boix-Adsera</b>, Matthew Brennan, Guy Bresler.
            <br>
            <em>Foundations of Computer Science (FOCS)</em>, 2019.
            <br>
            <b>Invited to the SIAM Journal on Computing Special Issue for FOCS 2019</b>
          </li>
          <li>
            <a href="https://dl.acm.org/doi/10.1145/3293611.3331593" class="publications"> Randomized Concurrent Set
              Union and Generalized Wake-Up</a>
            <br>
            Siddhartha Jayanti*, Robert E. Tarjan*, <b>Enric Boix-Adsera</b>.
            <br>
            <em>Symposium on Principles of Distributed Computing (PODC),</em> 2019.
          </li>

        </ul>
      </div>
    </div>


  </div>

  <br>
  <hr>
  <p style="font-size:0.9em"> Website design adapted from <a href="https://ai.stanford.edu/~tengyuma">Tengyu Ma</a>'s
</body>

</html>
<script>
  document.addEventListener('DOMContentLoaded', function () {
    var headers = document.querySelectorAll('.expandable-header');
    headers.forEach(function (header) {
      header.addEventListener('click', function () {
        this.classList.toggle('expanded');
        var content = this.nextElementSibling;
        if (content.style.maxHeight) {
          content.style.maxHeight = null;
        } else {
          content.style.maxHeight = content.scrollHeight + "px";
        }
      });
    });
  });
</script>
